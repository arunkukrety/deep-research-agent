# langgraph research agent

comprehensive ai research agent with web api that generates detailed reports using multiple llms and search tools.

## ğŸš€ features

- **3-stage pipeline**: query enhancer â†’ planner â†’ summarizer
- **dual llm setup**: groq (llama 3.1) for search planning, gemini for report writing
- **web search**: serper api for google search results
- **content crawling**: exa api for full article extraction
- **fastapi endpoint**: `/research?q=query` returns markdown reports
- **token optimization**: prevents groq context overflow
- **detailed reports**: comprehensive analysis, not summaries
- **auto file saving**: saves reports to `output/` folder

## ğŸ“‹ requirements

- python 3.13+
- api keys:
  - **GROQ_API_KEY** - [groq console](https://console.groq.com/)
  - **SERPER_API_KEY** - [serper.dev](https://serper.dev/)
  - **EXA_API_KEY** - [exa.ai](https://exa.ai/)
  - **GOOGLE_API_KEY** - [google ai studio](https://aistudio.google.com/)

## ğŸ› ï¸ installation

### using uv (recommended)
```bash
git clone <repo-url>
cd langgraph-agent
uv sync
```

### using pip
```bash
git clone <repo-url>
cd langgraph-agent
pip install -r requirements.txt
```

### for api usage
```bash
# add api dependencies
uv add fastapi uvicorn
# or with pip
pip install fastapi uvicorn
```

## âš™ï¸ setup

create `.env` file:
```env
GROQ_API_KEY=your_key_here
SERPER_API_KEY=your_key_here  
EXA_API_KEY=your_key_here
GOOGLE_API_KEY=your_key_here
```

## ğŸš€ usage

### api mode (recommended)
```bash
python api.py
```
then visit: `http://localhost:8000/research?q=your-query`

### command line mode
```bash
python main.py
```

### programmatic usage
```python
from main import graph_builder

agent = graph_builder()
result = agent.invoke({"user_input": "ai trends 2024"})
print(result["llm_response"])
```

## ğŸ—ï¸ structure

```
langgraph-agent/
â”œâ”€â”€ agents/                 # research pipeline
â”‚   â”œâ”€â”€ query_enhancer.py   # improves user queries
â”‚   â”œâ”€â”€ planner.py          # searches + selects urls
â”‚   â””â”€â”€ summarizer.py       # writes final reports
â”œâ”€â”€ tools/                  # search tools
â”‚   â”œâ”€â”€ serper_search.py    # google search via serper
â”‚   â””â”€â”€ exa_search.py       # content crawling via exa
â”œâ”€â”€ utils/                  # shared utilities
â”‚   â”œâ”€â”€ llm.py             # groq + gemini clients
â”‚   â””â”€â”€ prompts.py         # agent prompts
â”œâ”€â”€ output/                # saved reports
â”œâ”€â”€ main.py                # cli workflow
â”œâ”€â”€ api.py                 # fastapi web server
â”œâ”€â”€ pyproject.toml         # dependencies
â””â”€â”€ requirements.txt       # pip fallback
```

## ğŸŒ api endpoints

- `GET /` - health check
- `GET /research?q=query` - generates markdown research report
- `GET /docs` - api documentation

## ğŸ”§ how it works

1. **query enhancer** (gemini) - improves user query, generates research questions
2. **planner** (groq) - searches with serper, selects best urls, crawls with exa
3. **summarizer** (gemini) - writes comprehensive report from crawled content

## ğŸ“Š token optimization

- groq only processes lightweight search results (not full articles)
- exa crawling happens outside llm context
- gemini handles large content for final reports
- prevents 413 token limit errors

## ğŸš€ deployment

works on any python hosting platform:
- railway, render, heroku
- vercel, netlify functions  
- aws lambda, google cloud run
- local with ngrok for testing


